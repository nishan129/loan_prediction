{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.loan.constant.trainingpipeline import SCHEMA_FILE_PATH\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "from src.loan.exception import ModelException\n",
    "from src.loan.logger import logging\n",
    "from scipy.stats import ks_2samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.loan.utils.main_utils import read_yaml_file, write_yaml_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_config = read_yaml_file(\"D:/Machine Learning project/Loan/Loan Project/loan_prediction/config/schema.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "def read_yaml_file(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        data = yaml.safe_load(file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = read_yaml_file(\"D:/Machine Learning project/Loan/Loan Project/loan_prediction/config/schema.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_dataset_drift(base_df,current_df,threshold=0.05)-> bool:\n",
    "        try:\n",
    "            status = True\n",
    "            report = {}\n",
    "            for col in base_df.columns:\n",
    "                d1 = base_df[col]\n",
    "                d2 = current_df[col]\n",
    "                is_same_dist = ks_2samp(d1,d2)\n",
    "                if is_same_dist.pvalue >= threshold:\n",
    "                    is_found = False\n",
    "                else:\n",
    "                    is_found = True\n",
    "                    status = False\n",
    "                report.update({col:{\"p_value\": float(is_same_dist.pvalue),\n",
    "                                        \"drift_status\": is_found}})\n",
    "            drift_report_file_path = data_validation_config.drift_report_file_path\n",
    "            \n",
    "            file_path = os.path.dirname(drift_report_file_path)\n",
    "            os.makedirs(file_path,exist_ok=True)\n",
    "            write_yaml_file(filename=file_path,data=report)\n",
    "            return status\n",
    "        except Exception as e:\n",
    "            raise ModelException(e,sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.loan.constant.trainingpipeline import SCHEMA_FILE_PATH\n",
    "from src.loan.entity.artifact_entity import DataIngestionArtifact,DataValidationArtifact\n",
    "from src.loan.entity.config_entity import DataValidationConfig\n",
    "from src.loan.logger import logging\n",
    "from src.loan.utils.main_utils import read_yaml_file, write_yaml_file\n",
    "import sys,os\n",
    "from scipy.stats import ks_2samp\n",
    "from src.loan.exception import ModelException\n",
    "import pandas as pd\n",
    "class DataValidation:\n",
    "    \n",
    "    def __init__(self, data_ingenstion_artifact:DataIngestionArtifact,\n",
    "                 data_validation_config:DataValidationConfig):\n",
    "        try:\n",
    "            self.data_ingenstion_artifact = data_ingenstion_artifact\n",
    "            self.data_validation_config = data_validation_config\n",
    "            self.schema_config = read_yaml_file(SCHEMA_FILE_PATH)\n",
    "        except Exception as e:\n",
    "            raise ModelException(e,sys)\n",
    "        \n",
    "    def validate_number_of_columns(self,dataframe:pd.DataFrame) -> bool:\n",
    "        try:\n",
    "            numeber_of_colums = len(self.schema_config['columns'])\n",
    "            if len(dataframe.columns) == numeber_of_colums:\n",
    "                return True\n",
    "        except Exception as e:\n",
    "            raise ModelException(e,sys)\n",
    "        \n",
    "    def is_numerical_column_is_exist(self,dataframe:pd.DataFrame) -> bool:\n",
    "        try:\n",
    "            numerical_columns = self.schema_config[\"numerical_columns\"]\n",
    "            datacolumns = dataframe.columns\n",
    "            \n",
    "            numerical_columns_prasent = True\n",
    "            missing_numerical_columns = []\n",
    "            \n",
    "            for num_columns in numerical_columns:\n",
    "                if num_columns not in datacolumns:\n",
    "                    numerical_columns_prasent = False\n",
    "                    missing_numerical_columns.append(num_columns)\n",
    "            logging.info(f\"Missing numerical columns is {missing_numerical_columns}\")\n",
    "            return numerical_columns_prasent\n",
    "        except Exception as e:\n",
    "            raise ModelException(e,sys)\n",
    "        \n",
    "    def is_categorical_column_is_exist(self, datafram:pd.DataFrame) -> bool:\n",
    "        try:\n",
    "            categorical_columns = self.schema_config['categorical_columns']\n",
    "            data_columns = datafram.columns\n",
    "            \n",
    "            categorical_columns_present = True\n",
    "            missing_categorical_columns = []\n",
    "            \n",
    "            for cate_col in categorical_columns:\n",
    "                if cate_col not in data_columns:\n",
    "                    categorical_columns_present = False\n",
    "                    missing_categorical_columns.append(cate_col)\n",
    "            logging.info(f\"Missing categorical columns{missing_categorical_columns}\")\n",
    "            return categorical_columns_present\n",
    "        except Exception as e:\n",
    "            raise ModelException(e,sys)\n",
    "    \n",
    "    @staticmethod\n",
    "    def read_data(file_path) -> pd.DataFrame:\n",
    "        try:\n",
    "            return pd.read_parquet(file_path)\n",
    "        except Exception as e:\n",
    "            raise ModelException(e,sys)\n",
    "        \n",
    "    def detect_dataset_drift(self,base_df,current_df,threshold=0.05)-> bool:\n",
    "        try:\n",
    "            status = True\n",
    "            report = {}\n",
    "            for col in base_df.columns:\n",
    "                d1 = base_df[col]\n",
    "                d2 = current_df[col]\n",
    "                is_same_dist = ks_2samp(d1,d2)\n",
    "                if is_same_dist.pvalue >= threshold:\n",
    "                    is_found = False\n",
    "                else:\n",
    "                    is_found = True\n",
    "                    status = False\n",
    "                report.update({col:{\"p_value\": float(is_same_dist.pvalue),\n",
    "                                        \"drift_status\": is_found}})\n",
    "            drift_report_file_path = self.data_validation_config.drift_report_file_path\n",
    "            \n",
    "            file_path = os.path.dirname(drift_report_file_path)\n",
    "            os.makedirs(file_path,exist_ok=True)\n",
    "            write_yaml_file(filename=file_path,data=report)\n",
    "            return status\n",
    "        except Exception as e:\n",
    "            raise ModelException(e,sys)\n",
    "        \n",
    "    def initiate_data_validation(self)-> DataValidationArtifact:\n",
    "        try:\n",
    "            error_massage = \"\"\n",
    "            train_file_path = self.data_ingenstion_artifact.trained_file_path\n",
    "            test_file_path = self.data_ingenstion_artifact.test_file_path\n",
    "            \n",
    "            # Reading data from train and test file path\n",
    "            train_dataframe = DataValidation.read_data(train_file_path)\n",
    "            test_dataframe = DataValidation.read_data(test_file_path)\n",
    "            \n",
    "            # Validate number of columns\n",
    "            \n",
    "            status = self.validate_number_of_columns(dataframe=train_dataframe)\n",
    "            if not status:\n",
    "                error_massage = f\"{error_massage}Train dataframe does not contain all columns. \"\n",
    "            \n",
    "            status = self.validate_number_of_columns(dataframe=test_dataframe)\n",
    "            if not status:\n",
    "                error_massage = f\"{error_massage} Test dataframe does not contain all columns. \"\n",
    "            \n",
    "            status = self.is_numerical_column_is_exist(dataframe=train_dataframe)\n",
    "            if not status:\n",
    "                error_massage = f\"{error_massage} Train dataframe does not contain all numerical columns. \"\n",
    "        \n",
    "            status = self.is_numerical_column_is_exist(dataframe=test_dataframe)\n",
    "            if not status:\n",
    "                error_massage = f\"{error_massage} Test dataframe does not contain all numerical columns. \"\n",
    "            \n",
    "            status = self.is_categorical_column_is_exist(datafram=train_dataframe)\n",
    "            if not status:\n",
    "                error_massage = f\"{error_massage} Train dataframe does not contain all categorical columns. \" \n",
    "            \n",
    "            status = self.is_categorical_column_is_exist(datafram=test_dataframe)\n",
    "            if not status:\n",
    "                error_massage = f\"{error_massage} Test dataframe does not contain all categorical columns. \"\n",
    "            \n",
    "            if len(error_massage) > 0:\n",
    "                raise Exception(error_massage)\n",
    "             \n",
    "            # Lets check data drift \n",
    "            status = self.detect_dataset_drift(base_df=train_dataframe,current_df=test_dataframe)\n",
    "            \n",
    "            # To get artifcats folder\n",
    "            data_validation_artifacts = DataValidationArtifact(\n",
    "                validation_status=status,valid_train_file_path=self.data_ingenstion_artifact.trained_file_path,\n",
    "                valid_test_file_path=self.data_ingenstion_artifact.test_file_path,\n",
    "                invalid_train_file_path=self.data_validation_config.invalid_train_file_path,\n",
    "                invalid_test_file_path=self.data_validation_config.invalid_test_file_path,\n",
    "                drift_report_file_path=self.data_validation_config.drift_report_file_path\n",
    "            )\n",
    "            logging.info(f\"Data validation artifact: {data_validation_artifacts}\")\n",
    "        except Exception as e:\n",
    "            raise ModelException(e,sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"D:/Machine Learning project/Loan/Loan Project/loan_prediction/research\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = {\"not\":\"ok\",\"oy\":\"jok\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_yaml_file(filename: str, data, replace:bool = False) -> None:\n",
    "    try:\n",
    "        if replace:\n",
    "            if os.path.exists(filename):\n",
    "                os.remove(filename)\n",
    "        os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "        with open(filename, \"w\") as file:\n",
    "            return yaml.dump(data,file,default_flow_style=False)\n",
    "    except Exception as e:\n",
    "        raise ModelException(e,sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModelException",
     "evalue": "Error occurred python script name [C:\\Users\\Nishant Borkar\\AppData\\Local\\Temp\\ipykernel_21704\\1900489292.py] line number [7] error massage [[Errno 13] Permission denied: 'D:/Machine Learning project/Loan/Loan Project/loan_prediction/research']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 7\u001b[0m, in \u001b[0;36mwrite_yaml_file\u001b[1;34m(filename, data, replace)\u001b[0m\n\u001b[0;32m      6\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(filename), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m yaml\u001b[38;5;241m.\u001b[39mdump(data,file,default_flow_style\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Nishant Borkar\\anaconda3\\envs\\lven\\lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'D:/Machine Learning project/Loan/Loan Project/loan_prediction/research'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mModelException\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mwrite_yaml_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[54], line 10\u001b[0m, in \u001b[0;36mwrite_yaml_file\u001b[1;34m(filename, data, replace)\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m yaml\u001b[38;5;241m.\u001b[39mdump(data,file,default_flow_style\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 10\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ModelException(e,sys)\n",
      "\u001b[1;31mModelException\u001b[0m: Error occurred python script name [C:\\Users\\Nishant Borkar\\AppData\\Local\\Temp\\ipykernel_21704\\1900489292.py] line number [7] error massage [[Errno 13] Permission denied: 'D:/Machine Learning project/Loan/Loan Project/loan_prediction/research']"
     ]
    }
   ],
   "source": [
    "write_yaml_file(filename=filename,data=report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writ_yaml_file(data, filename):\n",
    "    with open(filename, 'w') as file:\n",
    "        yaml.dump(data, file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'D:/Machine Learning project/Loan/Loan Project/loan_prediction/research'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mwrit_yaml_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[57], line 2\u001b[0m, in \u001b[0;36mwrit_yaml_file\u001b[1;34m(data, filename)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrit_yaml_file\u001b[39m(data, filename):\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      3\u001b[0m         yaml\u001b[38;5;241m.\u001b[39mdump(data, file, default_flow_style\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Nishant Borkar\\anaconda3\\envs\\lven\\lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'D:/Machine Learning project/Loan/Loan Project/loan_prediction/research'"
     ]
    }
   ],
   "source": [
    "writ_yaml_file(data=report,filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lven",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
